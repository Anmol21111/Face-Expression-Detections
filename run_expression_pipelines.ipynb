{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45ea3b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≠ COMPLETE FACIAL EXPRESSION RECOGNITION SYSTEM\n",
      "============================================================\n",
      "\n",
      "üìã MAIN MENU:\n",
      "1. üé® Create Dataset (Synthetic - Instant)\n",
      "2. üß† Train Model\n",
      "3. üîç Real-time Detection\n",
      "4. üöÄ Run Complete Pipeline\n",
      "0. ‚ùå Exit\n",
      "\n",
      "Enter your choice (0-4): 3\n",
      "\n",
      "üöÄ Starting real-time detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Facial expression model loaded successfully!\n",
      "üöÄ Starting real-time facial expression detection...\n",
      "üéÆ Press 'Q' to quit, 'S' to save image\n",
      "‚úÖ Real-time detection stopped.\n",
      "\n",
      "üìã MAIN MENU:\n",
      "1. üé® Create Dataset (Synthetic - Instant)\n",
      "2. üß† Train Model\n",
      "3. üîç Real-time Detection\n",
      "4. üöÄ Run Complete Pipeline\n",
      "0. ‚ùå Exit\n",
      "\n",
      "Enter your choice (0-4): 0\n",
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==================== DATASET COLLECTOR ====================\n",
    "class DatasetCollector:\n",
    "    def __init__(self, target_images=500):\n",
    "        self.expressions = {\n",
    "            '1': 'angry',\n",
    "            '2': 'disgust', \n",
    "            '3': 'fear',\n",
    "            '4': 'happy',\n",
    "            '5': 'neutral',\n",
    "            '6': 'sad',\n",
    "            '7': 'surprise'\n",
    "        }\n",
    "        self.dataset_dir = 'dataset'\n",
    "        self.target_images = target_images\n",
    "        self.face_cascade = cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "        )\n",
    "        self.current_expression = 'neutral'\n",
    "        self.counters = {expr: 0 for expr in self.expressions.values()}\n",
    "        self.cap = None\n",
    "        self.running = True\n",
    "        \n",
    "        self.setup_directories()\n",
    "    \n",
    "    def setup_directories(self):\n",
    "        \"\"\"Create dataset directory structure\"\"\"\n",
    "        print(\"üìÅ Creating dataset directories...\")\n",
    "        os.makedirs(self.dataset_dir, exist_ok=True)\n",
    "        for expression in self.expressions.values():\n",
    "            expr_dir = os.path.join(self.dataset_dir, expression)\n",
    "            os.makedirs(expr_dir, exist_ok=True)\n",
    "        print(f\"‚úÖ Created directories for {len(self.expressions)} expressions\")\n",
    "    \n",
    "    def capture_dataset(self):\n",
    "        \"\"\"Create dataset instantly\"\"\"\n",
    "        print(\"üöÄ Creating facial expression dataset...\")\n",
    "        print(f\"üéØ Target: {self.target_images} images per expression\")\n",
    "        \n",
    "        for expression in self.expressions.values():\n",
    "            print(f\"üé≠ Generating {expression} images...\")\n",
    "            expr_dir = os.path.join(self.dataset_dir, expression)\n",
    "            \n",
    "            for i in tqdm(range(self.target_images), desc=f\"  {expression}\"):\n",
    "                # Create synthetic face image\n",
    "                img = self.create_dataset(expression)\n",
    "                \n",
    "                # Save image\n",
    "                filename = f\"{expression}_{i:04d}.jpg\"\n",
    "                filepath = os.path.join(expr_dir, filename)\n",
    "                cv2.imwrite(filepath, img)\n",
    "                \n",
    "                self.counters[expression] += 1\n",
    "            \n",
    "            print(f\"‚úÖ {expression}: {self.target_images} images created\")\n",
    "        \n",
    "        total_images = sum(self.counters.values())\n",
    "        print(f\"\\nüéâ Dataset created successfully!\")\n",
    "        print(f\"üìä Total images: {total_images}\")\n",
    "        print(f\"üíæ Location: {self.dataset_dir}/\")\n",
    "    \n",
    "    def create_dataset(self, expression):\n",
    "        \"\"\"Create synthetic face image with expression\"\"\"\n",
    "        img_size = 48\n",
    "        image = np.zeros((img_size, img_size), dtype=np.uint8)\n",
    "        center_x, center_y = img_size // 2, img_size // 2\n",
    "        \n",
    "        # Draw face oval\n",
    "        face_radius = random.randint(18, 22)\n",
    "        cv2.circle(image, (center_x, center_y), face_radius, 255, -1)\n",
    "        \n",
    "        # Add expression features\n",
    "        if expression == 'angry':\n",
    "            # Angry eyes and mouth\n",
    "            cv2.circle(image, (center_x-8, center_y-4), 2, 0, -1)\n",
    "            cv2.circle(image, (center_x+8, center_y-4), 2, 0, -1)\n",
    "            cv2.line(image, (center_x-6, center_y+8), (center_x+6, center_y+8), 0, 2)\n",
    "        elif expression == 'happy':\n",
    "            # Happy eyes and smile\n",
    "            cv2.circle(image, (center_x-8, center_y-4), 2, 0, -1)\n",
    "            cv2.circle(image, (center_x+8, center_y-4), 2, 0, -1)\n",
    "            cv2.ellipse(image, (center_x, center_y+8), (6, 3), 0, 0, 180, 0, 2)\n",
    "        elif expression == 'sad':\n",
    "            # Sad eyes and mouth\n",
    "            cv2.circle(image, (center_x-8, center_y-4), 2, 0, -1)\n",
    "            cv2.circle(image, (center_x+8, center_y-4), 2, 0, -1)\n",
    "            cv2.ellipse(image, (center_x, center_y+10), (5, 2), 0, 180, 360, 0, 2)\n",
    "        elif expression == 'surprise':\n",
    "            # Surprised eyes and mouth\n",
    "            cv2.circle(image, (center_x-8, center_y-4), 3, 0, -1)\n",
    "            cv2.circle(image, (center_x+8, center_y-4), 3, 0, -1)\n",
    "            cv2.circle(image, (center_x, center_y+8), 3, 0, 2)\n",
    "        else:  # neutral, disgust, fear\n",
    "            cv2.circle(image, (center_x-8, center_y-4), 2, 0, -1)\n",
    "            cv2.circle(image, (center_x+8, center_y-4), 2, 0, -1)\n",
    "            cv2.line(image, (center_x-5, center_y+8), (center_x+5, center_y+8), 0, 2)\n",
    "        \n",
    "        # Add random variations\n",
    "        image = self.add_variations(image)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def add_variations(self, image):\n",
    "        \"\"\"Add random variations to images\"\"\"\n",
    "        # Brightness variation\n",
    "        brightness = random.uniform(0.8, 1.2)\n",
    "        image = cv2.convertScaleAbs(image, alpha=brightness)\n",
    "        \n",
    "        # Noise\n",
    "        noise = np.random.randint(-5, 5, image.shape, dtype=np.int16)\n",
    "        image = np.clip(image.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        return image\n",
    "\n",
    "# ==================== MODEL TRAINER ====================\n",
    "class ExpressionTrainer:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.img_size = 48\n",
    "        self.num_classes = 7\n",
    "        self.class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "    \n",
    "    def create_model(self):\n",
    "        \"\"\"Create CNN model for facial expression recognition\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(self.img_size, self.img_size, 1)),\n",
    "            layers.MaxPooling2D(2, 2),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D(2, 2),\n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D(2, 2),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(self.num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def load_dataset(self, data_path):\n",
    "        \"\"\"Load and prepare dataset\"\"\"\n",
    "        print(\"üìä Loading dataset...\")\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for class_idx, expression in enumerate(self.class_names):\n",
    "            expr_path = os.path.join(data_path, expression)\n",
    "            if not os.path.exists(expr_path):\n",
    "                print(f\"‚ùå Directory not found: {expr_path}\")\n",
    "                continue\n",
    "            \n",
    "            image_files = [f for f in os.listdir(expr_path) if f.endswith('.jpg')]\n",
    "            print(f\"üìÅ {expression}: {len(image_files)} images\")\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                img_path = os.path.join(expr_path, img_file)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                if img is not None:\n",
    "                    # Resize and normalize\n",
    "                    img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "                    img = img.astype('float32') / 255.0\n",
    "                    \n",
    "                    X.append(img)\n",
    "                    y.append(class_idx)\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            print(\"‚ùå No images found in dataset!\")\n",
    "            return None, None\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Reshape for CNN\n",
    "        X = X.reshape(-1, self.img_size, self.img_size, 1)\n",
    "        \n",
    "        # Convert labels to categorical\n",
    "        y = keras.utils.to_categorical(y, self.num_classes)\n",
    "        \n",
    "        print(f\"‚úÖ Dataset loaded: {X.shape[0]} images\")\n",
    "        return X, y\n",
    "    \n",
    "    def train(self, data_path, epochs=20):\n",
    "        \"\"\"Train the facial expression model\"\"\"\n",
    "        print(\"üß† Starting model training...\")\n",
    "        \n",
    "        # Load dataset\n",
    "        X, y = self.load_dataset(data_path)\n",
    "        if X is None:\n",
    "            return False\n",
    "        \n",
    "        # Split dataset\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        print(f\"üìä Training samples: {X_train.shape[0]}\")\n",
    "        print(f\"üìä Validation samples: {X_test.shape[0]}\")\n",
    "        \n",
    "        # Create model\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        print(\"üìã Model architecture:\")\n",
    "        self.model.summary()\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5)\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        print(\"üöÄ Training model...\")\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_test, y_test),\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        self.model.save('models/expression_model.h5')\n",
    "        print(\"‚úÖ Model saved as: models/expression_model.h5\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        test_loss, test_accuracy = self.model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f\"üìä Final Validation Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"üìä Final Validation Loss: {test_loss:.4f}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "# ==================== REAL-TIME DETECTOR ====================\n",
    "class RealTimeDetector:\n",
    "    def __init__(self):\n",
    "        self.face_cascade = cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "        )\n",
    "        self.expression_model = None\n",
    "        self.img_size = 48\n",
    "        self.class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "        \n",
    "        # Colors for expressions\n",
    "        self.expression_colors = {\n",
    "            'angry': (0, 0, 255),        # Red\n",
    "            'disgust': (0, 128, 0),      # Dark Green\n",
    "            'fear': (128, 0, 128),       # Purple\n",
    "            'happy': (0, 255, 255),      # Yellow\n",
    "            'neutral': (255, 255, 255),  # White\n",
    "            'sad': (255, 0, 0),          # Blue\n",
    "            'surprise': (0, 165, 255)    # Orange\n",
    "        }\n",
    "        \n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the trained expression model\"\"\"\n",
    "        model_path = 'models/expression_model.h5'\n",
    "        if os.path.exists(model_path):\n",
    "            try:\n",
    "                self.expression_model = keras.models.load_model(model_path)\n",
    "                print(\"‚úÖ Facial expression model loaded successfully!\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading model: {e}\")\n",
    "        \n",
    "        print(\"‚ùå No trained model found. Please train the model first.\")\n",
    "        return False\n",
    "    \n",
    "    def detect_expressions(self):\n",
    "        \"\"\"Real-time facial expression detection\"\"\"\n",
    "        if self.expression_model is None:\n",
    "            print(\"‚ùå Cannot start detection without a trained model.\")\n",
    "            return\n",
    "        \n",
    "        # Initialize camera\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(\"‚ùå Error: Could not open camera\")\n",
    "            return\n",
    "        \n",
    "        print(\"üöÄ Starting real-time facial expression detection...\")\n",
    "        print(\"üéÆ Press 'Q' to quit, 'S' to save image\")\n",
    "        \n",
    "        prev_time = time.time()\n",
    "        fps = 0\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Calculate FPS\n",
    "                current_time = time.time()\n",
    "                fps = 1.0 / (current_time - prev_time)\n",
    "                prev_time = current_time\n",
    "                \n",
    "                # Flip frame for mirror effect\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Convert to grayscale\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Detect faces\n",
    "                faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "                \n",
    "                # Process each face\n",
    "                for (x, y, w, h) in faces:\n",
    "                    # Extract face ROI\n",
    "                    face_roi = gray[y:y+h, x:x+w]\n",
    "                    \n",
    "                    # Preprocess for model\n",
    "                    processed_face = cv2.resize(face_roi, (self.img_size, self.img_size))\n",
    "                    processed_face = processed_face.astype('float32') / 255.0\n",
    "                    processed_face = processed_face.reshape(1, self.img_size, self.img_size, 1)\n",
    "                    \n",
    "                    # Predict expression\n",
    "                    predictions = self.expression_model.predict(processed_face, verbose=0)\n",
    "                    predicted_class = np.argmax(predictions[0])\n",
    "                    confidence = predictions[0][predicted_class]\n",
    "                    expression = self.class_names[predicted_class]\n",
    "                    \n",
    "                    # Get color for expression\n",
    "                    color = self.expression_colors.get(expression, (255, 255, 255))\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), color, 3)\n",
    "                    \n",
    "                    # Draw label\n",
    "                    label = f\"{expression} ({confidence:.2f})\"\n",
    "                    cv2.putText(frame, label, (x, y-10), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "                \n",
    "                # Display FPS\n",
    "                cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "                # Display instructions\n",
    "                cv2.putText(frame, \"Press 'Q' to quit\", (10, 60), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)\n",
    "                \n",
    "                # Show frame\n",
    "                cv2.imshow('Real-time Facial Expression Detection', frame)\n",
    "                \n",
    "                # Handle key press\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "                elif key == ord('s'):\n",
    "                    # Save current frame\n",
    "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    filename = f\"detection_{timestamp}.jpg\"\n",
    "                    cv2.imwrite(filename, frame)\n",
    "                    print(f\"üíæ Saved: {filename}\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n‚èπÔ∏è Detection stopped by user\")\n",
    "        \n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            print(\"‚úÖ Real-time detection stopped.\")\n",
    "\n",
    "# ==================== MAIN PIPELINE ====================\n",
    "def main():\n",
    "    print(\"üé≠ COMPLETE FACIAL EXPRESSION RECOGNITION SYSTEM\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create necessary directories\n",
    "    os.makedirs('dataset', exist_ok=True)\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nüìã MAIN MENU:\")\n",
    "        print(\"1. üé® Create Dataset (Synthetic - Instant)\")\n",
    "        print(\"2. üß† Train Model\")\n",
    "        print(\"3. üîç Real-time Detection\")\n",
    "        print(\"4. üöÄ Run Complete Pipeline\")\n",
    "        print(\"0. ‚ùå Exit\")\n",
    "        \n",
    "        choice = input(\"\\nEnter your choice (0-4): \").strip()\n",
    "        \n",
    "        if choice == '1':\n",
    "            print(\"\\nüöÄ Creating synthetic dataset...\")\n",
    "            try:\n",
    "                target = int(input(\"Enter images per expression (default 500): \") or \"500\")\n",
    "                collector = DatasetCollector(target_images=target)\n",
    "                collector.capture_dataset()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error: {e}\")\n",
    "        \n",
    "        elif choice == '2':\n",
    "            print(\"\\nüöÄ Training model...\")\n",
    "            try:\n",
    "                trainer = ExpressionTrainer()\n",
    "                trainer.train('dataset', epochs=2)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error: {e}\")\n",
    "        \n",
    "        elif choice == '3':\n",
    "            print(\"\\nüöÄ Starting real-time detection...\")\n",
    "            try:\n",
    "                detector = RealTimeDetector()\n",
    "                detector.detect_expressions()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error: {e}\")\n",
    "        \n",
    "        elif choice == '4':\n",
    "            print(\"\\nüöÄ RUNNING COMPLETE PIPELINE...\")\n",
    "            try:\n",
    "                # Step 1: Create dataset\n",
    "                print(\"\\nüìù Step 1: Creating Dataset\")\n",
    "                collector = DatasetCollector(target_images=500)\n",
    "                collector.capture_dataset()\n",
    "                \n",
    "                # Step 2: Train model\n",
    "                print(\"\\nüìù Step 2: Training Model\")\n",
    "                trainer = ExpressionTrainer()\n",
    "                success = trainer.train('dataset', epochs=10)\n",
    "                \n",
    "                if success:\n",
    "                    # Step 3: Real-time detection\n",
    "                    print(\"\\nüìù Step 3: Real-time Detection\")\n",
    "                    detector = RealTimeDetector()\n",
    "                    detector.detect_expressions()\n",
    "                else:\n",
    "                    print(\"‚ùå Model training failed. Cannot start detection.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Pipeline error: {e}\")\n",
    "        \n",
    "        elif choice == '0':\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå Invalid choice. Please try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4af5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
